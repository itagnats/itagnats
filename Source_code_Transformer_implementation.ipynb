{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORqSXVMEUOgzwvp54gEsE9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Transformer-based models Implementation**\n",
        "โมเดลหลักที่ใช้ในงานวิจัย\\\n",
        "Bert, ALBERT, RoBERTa, TF-IDF\n",
        "\n"
      ],
      "metadata": {
        "id": "ByQ4XLDyTn4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import Libraries**"
      ],
      "metadata": {
        "id": "eFuwSfekTz5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBy6IzyeTlaE"
      },
      "outputs": [],
      "source": [
        "# require GPU to run transformer model\n",
        "import matplotlib\n",
        "print(matplotlib.__version__)\n",
        "!pip -q install torch==1.5.0 torchtext==0.4.0 torchvision==0.6.0\n",
        "!pip -q install transformers==3.5.0\n",
        "\n",
        "from transformers import (AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# SET PATH TO DATA FOLDER\n",
        "path= \"/content/drive/My Drive/Colab Notebooks/NLP_ITM/Research/\"\n",
        "\n",
        "import torch\n",
        "# check GPU available?\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "DakthkWMT8RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries for dataset preparation, feature engineering, model training\n",
        "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                    NearMiss, \n",
        "                                    InstanceHardnessThreshold,\n",
        "                                    CondensedNearestNeighbour,\n",
        "                                    EditedNearestNeighbours,\n",
        "                                    RepeatedEditedNearestNeighbours,\n",
        "                                    AllKNN,\n",
        "                                    NeighbourhoodCleaningRule,\n",
        "                                    OneSidedSelection,\n",
        "                                    TomekLinks)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline\n",
        "import pandas as pd, numpy, string\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Remove Special Charactors\n",
        "import re\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# import dataframe libraries\n",
        "!pip install pyspark\n",
        "!pip install koalas\n",
        "import databricks.koalas as ks\n",
        "from pyspark.sql import SparkSession\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "GbIt3vZiUNyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Visualization**"
      ],
      "metadata": {
        "id": "h7gpislVUdSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ks_df = ks.from_pandas(pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP_ITM/Research/ratings_and_sentiments UTF-8.csv', encoding = 'utf8'))\n",
        "ks_df.head()"
      ],
      "metadata": {
        "id": "WJew5urNUbvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word cloud Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import gc\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers, losses, optimizers"
      ],
      "metadata": {
        "id": "SZIPlzMzUjpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text preprocessing - Data cleaning\n",
        "def text_preprocessing(text, for_vec_models=False):\n",
        "    if for_vec_models:\n",
        "        text = text.lower()\n",
        "        text = re.sub('[^a-z]+', ' ', text)\n",
        "        text = text.strip()\n",
        "    else:\n",
        "        text = text.lower()\n",
        "        text = re.sub('[^a-z]+', ' ', text)\n",
        "        text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))\n",
        "        text = ' '.join(PorterStemmer().stem(word) for word in text.split())\n",
        "        text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "p4XQsVZFU2Ab"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stop words and process text\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texts = ks_df.review_text.apply(text_preprocessing)\n",
        "\n",
        "#Add text clean into new dataset\n",
        "\n",
        "texts_new = []\n",
        "for t in ks_df.review_text:\n",
        "    texts_new.append(text_preprocessing(t))\n",
        "\n",
        "ks_df['text_clean'] = texts_new\n",
        "ks_df.head()"
      ],
      "metadata": {
        "id": "I5_7b-1sU2oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_len = []\n",
        "for text in ks_df.text_clean:\n",
        "    review_text_len = len(text.split())\n",
        "    text_len.append(review_text_len)\n",
        "\n",
        "# announce text len\n",
        "ks_df['text_len'] = text_len"
      ],
      "metadata": {
        "id": "iUpo6pBcVZZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud and word len plotting\n",
        "# High Sentiment class\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.histplot(ks_df[ks_df[\"cat_rating\"]==\"HIGH\"][\"text_len\"],color=\"salmon\")\n",
        "plt.title(\"Distribution of Review text length for HIGH\")\n",
        "display(ks_df.text_len[ks_df[\"cat_rating\"]==\"HIGH\"].describe())\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "plt.figure(figsize=(20,20))\n",
        "wc = WordCloud(max_words=2000,min_font_size=10, height=800,width=1600,\n",
        "               background_color=\"white\").generate(\" \".join(ks_df[ks_df[\"cat_rating\"]==\"HIGH\"].text_clean))\n",
        "plt.imshow(wc)\n",
        "\n"
      ],
      "metadata": {
        "id": "X1BtR0hyVh9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud and word len plotting\n",
        "# Low Sentiment class\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.histplot(ks_df[ks_df[\"cat_rating\"]==\"LOW\"][\"text_len\"],color=\"salmon\")\n",
        "plt.title(\"Distribution of Review text length for LOW\")\n",
        "display(ks_df.text_len[ks_df[\"cat_rating\"]==\"LOW\"].describe())\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "plt.figure(figsize=(20,20))\n",
        "wc = WordCloud(max_words=2000,min_font_size=10, height=800,width=1600,\n",
        "               background_color=\"white\").generate(\" \".join(ks_df[ks_df[\"cat_rating\"]==\"LOW\"].text_clean))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "okaSWRfqVw4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud and word len plotting\n",
        "# Overall dataset\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "plt.figure(figsize=(20,20))\n",
        "wc = WordCloud(max_words=2000,min_font_size=10, height=800,width=1600,\n",
        "               background_color=\"white\").generate(\" \".join(ks_df.text_clean))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "2D-M8wGmV1ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preparation**"
      ],
      "metadata": {
        "id": "HY1mU-IGV84s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# load the data set\n",
        "data = pd.read_csv(path + '/ratings_and_sentiments UTF-8.csv')\n",
        "  \n",
        "# print info about columns in the dataframe\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "jAHMGH9UWBM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class measurement\n",
        "# High and Low classes\n",
        "\n",
        "data['bool_HIGH'].value_counts().plot.bar()\n",
        "\n",
        "# 1-5 star classes\n",
        "data['num_rating'].value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "c0XBrqw8WDTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imbalance data handling techniques\n",
        "#Using Random Over Sampling \n",
        "\n",
        "data_zip = list(zip(data['review_text'], data['coffee_shop_name']))\n",
        "ros = RandomOverSampler(random_state=0, sampling_strategy=)\n",
        "ros_x, ros_y = ros.fit_resample(data_zip,data['bool_HIGH'])\n",
        "\n",
        "ros_data2 = [row[0] for row in ros_x]\n",
        "\n",
        "ros_data = pd.DataFrame(list(zip(ros_data2, ros_y)),\n",
        "               columns =['review_text', 'bool_HIGH'])\n",
        "\n",
        "#Classes measurement checking\n",
        "ros_data['bool_HIGH'].value_counts().plot.bar()\n",
        "print(ros_data.info())"
      ],
      "metadata": {
        "id": "vYjyeT9LWSbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocessing and Feature Extraction**"
      ],
      "metadata": {
        "id": "_pbMuBajWoHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT**"
      ],
      "metadata": {
        "id": "_lvA4KMOW8sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import BERT model and tokenizer\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\", output_hidden_states=True)\n",
        "bert_model = bert_model.to(device)"
      ],
      "metadata": {
        "id": "rayxNX01W77y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([102, 101]).long()\n",
        "c = torch.Tensor([1] * 2).long()\n",
        "def adjust_encoded_input(encoded_input):\n",
        "\n",
        "  # delete first and last separator token and splits to 510 tokens\n",
        "  input_ids_chunks = list(encoded_input['input_ids'][0][1:-1].split(510))\n",
        "  attention_mask_chunks = list(encoded_input['attention_mask'][0][1:-1].split(510))\n",
        "\n",
        "  for i in range(len(input_ids_chunks)):\n",
        "\n",
        "    # add 101 to the first and 102 to last element tonsor padding len to 512 for transformer model \n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], a ])\n",
        "    # shifting 101 102\n",
        "    input_ids_chunks[i] = torch.roll(input_ids_chunks[i], 1, 0)\n",
        "    \n",
        "    # padding len to 512 for transformer model\n",
        "    pad_len = 512 - input_ids_chunks[i].shape[0]\n",
        "    b = torch.Tensor([0] * pad_len).long()\n",
        "\n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], b])\n",
        "\n",
        "    if len(attention_mask_chunks[i]) == 510:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "    else:\n",
        "      d = torch.Tensor([0] * (pad_len)).long()\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], d])\n",
        "\n",
        "  input_ids = torch.stack(input_ids_chunks)\n",
        "  attention_mask = torch.stack(attention_mask_chunks)\n",
        "\n",
        "  input_dict = {\n",
        "      'input_ids': input_ids.long(),\n",
        "      'attention_mask': attention_mask.int()\n",
        "  }\n",
        "\n",
        "  return input_dict"
      ],
      "metadata": {
        "id": "c4976EbmWrQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokens will be converted to tensor then appllied padding method to fill every tensor to 512 words"
      ],
      "metadata": {
        "id": "JxeLXQEjXMLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding sample\n",
        "\n",
        "text = \"11/13/2016 Beautiful eccentric coffee shop with a library of peculiar books.  Swings, couches, and pillow corners for cuddle puddles.  Coffee with mint and ways you've never imagined coffee could be made. Try the matcha green tea with soy, creamiest matcha I've ever had.  First time here and already my favorite coffee bar so far. See all photos from Vicki Y. for The Factory - Cafe With a Soul\"\n",
        "\n",
        "encoded_input = bert_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ],
      "metadata": {
        "id": "4THG1U_NXbkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def extract_last_four_with_bert(input_text, feature_extractor):\n",
        "  encoded_input = bert_tokenizer(input_text, return_tensors='pt').to(device)\n",
        "  encoded_input = adjust_encoded_input(encoded_input)\n",
        "  # hidden_states = feature_extractor(**encoded_input)[0]\n",
        "  _, _, hidden_states = feature_extractor(**encoded_input)\n",
        "\n",
        "  # FOR MEAN CALCULATION BETWEEN TENSOR DIMENSION\n",
        "\n",
        "  last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "\n",
        "  cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "  cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "  if cat_sentence_embedding.shape[0] != 3072:\n",
        "\n",
        "    doc_embedding = torch.sum(cat_sentence_embedding, dim=0)\n",
        "  else:\n",
        "    doc_embedding = copy.copy(cat_sentence_embedding)\n",
        "\n",
        "  return doc_embedding.cpu().detach().numpy().astype('float64')"
      ],
      "metadata": {
        "id": "KJhHh_2UXjGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"11/13/2016 Beautiful eccentric coffee shop with a library of peculiar books.  Swings, couches, and pillow corners for cuddle puddles.  Coffee with mint and ways you've never imagined coffee could be made. Try the matcha green tea with soy, creamiest matcha I've ever had.  First time here and already my favorite coffee bar so far. See all photos from Vicki Y. for The Factory - Cafe With a Soul\"\n",
        "\n",
        "t_2 = extract_last_four_with_bert(text, bert_model)\n",
        "t_2"
      ],
      "metadata": {
        "id": "p3edGYD6Xlml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " len(t_2)"
      ],
      "metadata": {
        "id": "3ULFT4HMXnHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RoBERTa**"
      ],
      "metadata": {
        "id": "OWu5FdtSXobP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Roberta model and tokenizer\n",
        "#active GPU\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "import gc\n",
        "Roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "Roberta_model = RobertaModel.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
        "Roberta_model = Roberta_model.to(device)"
      ],
      "metadata": {
        "id": "UfLfgsj-XrXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([102, 101]).long()\n",
        "c = torch.Tensor([1] * 2).long()\n",
        "def adjust_encoded_input_roberta(encoded_input):\n",
        "\n",
        "  # delete first and last separator token and splits to 510 tokens\n",
        "  input_ids_chunks = list(encoded_input['input_ids'][0][1:-1].split(510))\n",
        "  attention_mask_chunks = list(encoded_input['attention_mask'][0][1:-1].split(510))\n",
        "\n",
        "  for i in range(len(input_ids_chunks)):\n",
        "\n",
        "    # add 101 to the first and 102 to last element tonsor padding len to 512 for transformer model \n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], a ])\n",
        "    # shifting 101 102\n",
        "    input_ids_chunks[i] = torch.roll(input_ids_chunks[i], 1, 0)\n",
        "    \n",
        "    # padding len to 512 for transformer model\n",
        "    pad_len = 512 - input_ids_chunks[i].shape[0]\n",
        "    b = torch.Tensor([0] * pad_len).long()\n",
        "\n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], b])\n",
        "\n",
        "    if len(attention_mask_chunks[i]) == 510:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "    else:\n",
        "      d = torch.Tensor([0] * (pad_len)).long()\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], d])\n",
        "\n",
        "  input_ids = torch.stack(input_ids_chunks)\n",
        "  attention_mask = torch.stack(attention_mask_chunks)\n",
        "\n",
        "  input_dict = {\n",
        "      'input_ids': input_ids.long(),\n",
        "      'attention_mask': attention_mask.int()\n",
        "  }\n",
        "\n",
        "  return input_dict"
      ],
      "metadata": {
        "id": "HYwB65Y1X1zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"11/13/2016 Beautiful eccentric coffee shop with a library of peculiar books.  Swings, couches, and pillow corners for cuddle puddles.  Coffee with mint and ways you've never imagined coffee could be made. Try the matcha green tea with soy, creamiest matcha I've ever had.  First time here and already my favorite coffee bar so far. See all photos from Vicki Y. for The Factory - Cafe With a Soul\"\n",
        "\n",
        "encoded_input = Roberta_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input_roberta(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ],
      "metadata": {
        "id": "Ovr2IJaYX3ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def extract_last_four_with_roberta(input_text, feature_extractor):\n",
        "\n",
        "  encoded_input = Roberta_tokenizer(input_text, return_tensors='pt').to(device)\n",
        "  encoded_input = adjust_encoded_input_roberta(encoded_input)\n",
        "  _, _, hidden_states = feature_extractor(**encoded_input)\n",
        "\n",
        "  # use only last 4 layers \n",
        "  last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "\n",
        "  # concat last 4 layers vectors then calculate mean between vectors\n",
        "  cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "  cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "  # if document only has 1 batch, no need to sum vector\n",
        "  if cat_sentence_embedding.shape[0] != 3072:\n",
        "\n",
        "    doc_embedding = torch.sum(cat_sentence_embedding, dim=0)\n",
        "  else:\n",
        "    doc_embedding = copy.copy(cat_sentence_embedding)\n",
        "  \n",
        "  return doc_embedding.cpu().detach().numpy().astype('float64')"
      ],
      "metadata": {
        "id": "BLNTj2jwYB47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"11/13/2016 Beautiful eccentric coffee shop with a library of peculiar books.  Swings, couches, and pillow corners for cuddle puddles.  Coffee with mint and ways you've never imagined coffee could be made. Try the matcha green tea with soy, creamiest matcha I've ever had.  First time here and already my favorite coffee bar so far. See all photos from Vicki Y. for The Factory - Cafe With a Soul\"\n",
        "\n",
        "t_2 = extract_last_four_with_roberta(text, Roberta_model)\n",
        "t_2"
      ],
      "metadata": {
        "id": "0L6tX9vLYDJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t_2)"
      ],
      "metadata": {
        "id": "YC1Ecs-OYDr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ALBERT**"
      ],
      "metadata": {
        "id": "t4XOn5B6YXOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ALBERT model and tokenizer\n",
        "# active GPU\n",
        "device = torch.device(\"cuda\")\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertModel, AlbertConfig\n",
        "import torch\n",
        "import gc\n",
        "albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "albert_model = AlbertModel.from_pretrained('albert-base-v2', output_hidden_states=True)\n",
        "albert_model = albert_model.to(device)"
      ],
      "metadata": {
        "id": "3rvyzC8iYZ48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([102, 101]).long()\n",
        "c = torch.Tensor([1] * 2).long()\n",
        "def adjust_encoded_input_albert(encoded_input):\n",
        "\n",
        "  # delete first and last separator token and splits to 510 tokens\n",
        "  input_ids_chunks = list(encoded_input['input_ids'][0][1:-1].split(510))\n",
        "  attention_mask_chunks = list(encoded_input['attention_mask'][0][1:-1].split(510))\n",
        "\n",
        "  for i in range(len(input_ids_chunks)):\n",
        "\n",
        "    # add 101 to the first and 102 to last element tonsor padding len to 512 for transformer model \n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], a ])\n",
        "    # shifting 101 102\n",
        "    input_ids_chunks[i] = torch.roll(input_ids_chunks[i], 1, 0)\n",
        "    \n",
        "    # padding len to 512 for transformer model\n",
        "    pad_len = 512 - input_ids_chunks[i].shape[0]\n",
        "    b = torch.Tensor([0] * pad_len).long()\n",
        "\n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], b])\n",
        "\n",
        "    if len(attention_mask_chunks[i]) == 510:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "    else:\n",
        "      d = torch.Tensor([0] * (pad_len)).long()\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], d])\n",
        "\n",
        "  input_ids = torch.stack(input_ids_chunks)\n",
        "  attention_mask = torch.stack(attention_mask_chunks)\n",
        "\n",
        "  input_dict = {\n",
        "      'input_ids': input_ids.long(),\n",
        "      'attention_mask': attention_mask.int()\n",
        "  }\n",
        "\n",
        "  return input_dict"
      ],
      "metadata": {
        "id": "U_i_BmCwYiwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"11/13/2016 Beautiful eccentric coffee shop with a library of peculiar books.  Swings, couches, and pillow corners for cuddle puddles.  Coffee with mint and ways you've never imagined coffee could be made. Try the matcha green tea with soy, creamiest matcha I've ever had.  First time here and already my favorite coffee bar so far. See all photos from Vicki Y. for The Factory - Cafe With a Soul\"\n",
        "\n",
        "encoded_input = albert_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input_albert(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ],
      "metadata": {
        "id": "TAXBSurFYkQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def extract_last_four_with_albert(input_text, feature_extractor):\n",
        "\n",
        "  encoded_input = albert_tokenizer(input_text, return_tensors='pt').to(device)\n",
        "  encoded_input = adjust_encoded_input_albert(encoded_input)\n",
        "  _, _, hidden_states = feature_extractor(**encoded_input)\n",
        "\n",
        "  # use only last 4 layers \n",
        "  last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "\n",
        "  # concat last 4 layers vectors then calculate mean between vectors\n",
        "  cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "  cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "  # if document only has 1 batch, no need to sum vector\n",
        "  if cat_sentence_embedding.shape[0] != 3072:\n",
        "\n",
        "    doc_embedding = torch.sum(cat_sentence_embedding, dim=0)\n",
        "  else:\n",
        "    doc_embedding = copy.copy(cat_sentence_embedding)\n",
        "  \n",
        "  return doc_embedding.cpu().detach().numpy().astype('float64')"
      ],
      "metadata": {
        "id": "DXvSvvpoYloa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"11/13/2016 Beautiful eccentric coffee shop with a library of peculiar books.  Swings, couches, and pillow corners for cuddle puddles.  Coffee with mint and ways you've never imagined coffee could be made. Try the matcha green tea with soy, creamiest matcha I've ever had.  First time here and already my favorite coffee bar so far. See all photos from Vicki Y. for The Factory - Cafe With a Soul\"\n",
        "\n",
        "t_2 = extract_last_four_with_albert(text, albert_model)\n",
        "t_2"
      ],
      "metadata": {
        "id": "nGEC_vjZYnOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Announce new Dataframe"
      ],
      "metadata": {
        "id": "wmXmszU8Yn1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ks_da_df = pd.read_csv(path + '/ratings_and_sentiments UTF-8.csv')\n",
        "#ks_df = pd.read_csv(path + '/sentiments_by_shop.csv')"
      ],
      "metadata": {
        "id": "Syer33fZYs3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model output extraction** - Extract last four layers"
      ],
      "metadata": {
        "id": "zu736Fr8YwKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "albert_vectors = []\n",
        "\n",
        "bert_vectors = []\n",
        "\n",
        "roberta_vectors = []\n",
        "\n",
        "\n",
        "#df_pos = ks_da_df[ks_da_df['bool_HIGH']]\n",
        "\n",
        "\n",
        "\n",
        "for idx, row in ros_data.iterrows():\n",
        "\n",
        "  text = row['review_text']\n",
        "\n",
        "\n",
        "  albert_vector = extract_last_four_with_albert(text, albert_model)\n",
        "\n",
        "  bert_vector = extract_last_four_with_bert(text, bert_model)\n",
        "\n",
        "  roberta_vector = extract_last_four_with_roberta(text, Roberta_model)\n",
        "\n",
        "  albert_vectors.append(albert_vector)\n",
        "\n",
        "  bert_vectors.append(bert_vector)\n",
        "\n",
        "  roberta_vectors.append(roberta_vector)\n",
        "\n",
        "  print(idx)\n",
        "\n",
        "\n",
        "ros_data['content_bert_vector'] = bert_vectors\n",
        "\n",
        "ros_data['content_albert_vector'] = albert_vectors\n",
        "\n",
        "ros_data['content_roberta_vector'] = roberta_vectors\n",
        "\n",
        "ros_data.to_pickle(\"/content/drive/MyDrive/Colab Notebooks/NLP_ITM/Research/ratings_and_sentiments UTF-8.pkl\")"
      ],
      "metadata": {
        "id": "cF-LN0wwY_35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extrac last four layers then saved into pickle file to save trained time and be ready to use"
      ],
      "metadata": {
        "id": "I6FjJYz_ZE2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "pKRUsc41ZWBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import saved file from pickle"
      ],
      "metadata": {
        "id": "6PPS0maPZcr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "ros_data = pickle.load( open( \"/content/drive/MyDrive/Colab Notebooks/NLP_ITM/Research/ratings_and_sentiments UTF-8.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "ObXVnpsFZcK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqXGq4vrZlFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vfCTWecXZDXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks_selected_df = ros_data\n",
        "\n",
        "# train test spilt 80/20 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['bool_HIGH']), test_size=0.2, random_state=0)\n",
        "\n",
        "task = {\n",
        "    \"ks_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (LR)'\n",
        "    },\n",
        "    \"kh_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_albert_vector',\n",
        "        'language_model' : 'ALBERT (LR)'\n",
        "    },\n",
        "   \"ka_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_roberta_vector',\n",
        "        'language_model' : 'RoBERTa (LR)'\n",
        "         },\n",
        "}"
      ],
      "metadata": {
        "id": "N4NmNXyoZs_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task:\n",
        "\n",
        "  col = task[i]['col']\n",
        "  X = list(task[i]['data'][col])\n",
        "  y = list(task[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    logreg_model = LogisticRegression(max_iter=max_num_iter, random_state=0,multi_class='multinomial')\n",
        "    \n",
        "    logreg_model.fit(X, y)\n",
        "    y_pred = logreg_model.predict(X_test)\n",
        "    print(task[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "ukYDjN7PZxpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Logistic Regression HPOs**"
      ],
      "metadata": {
        "id": "yRHAN9lBZ3uH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - LR"
      ],
      "metadata": {
        "id": "aEP4qR9zaANe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR_parameters ={\n",
        "    'C': [10,20,30]} "
      ],
      "metadata": {
        "id": "lVmF_gb_Z-lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task:\n",
        "\n",
        "  col = task[i]['col']\n",
        "  X = list(task[i]['data'][col])\n",
        "  y = list(task[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = LogisticRegression()\n",
        "    model = GridSearchCV(clf, param_grid=LR_parameters, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    \n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "_8iGpIJ6aDmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "Kv7mDIBHaMYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Search - LR"
      ],
      "metadata": {
        "id": "wfDHWwFJaGOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task:\n",
        "\n",
        "  col = task[i]['col']\n",
        "  X = list(task[i]['data'][col])\n",
        "  y = list(task[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = LogisticRegression()\n",
        "    model = RandomizedSearchCV(clf, LR_parameters, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "NrMXw5BVaI5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "f9KOdGJBaM9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "aIn9hrTCaRh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pVFURyJLaOf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks_selected_df = ros_data\n",
        "\n",
        "# train test spilt 80/20 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['bool_HIGH']), test_size=0.2, random_state=0)\n",
        "\n",
        "task2 = {\n",
        "    \"ks_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (RF)'\n",
        "    },\n",
        "    \"kh_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_albert_vector',\n",
        "        'language_model' : 'ALBERT (RF)'\n",
        "    },\n",
        "   \"ka_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_roberta_vector',\n",
        "        'language_model' : 'RoBERTa (RF)'\n",
        "         },\n",
        "}"
      ],
      "metadata": {
        "id": "Egr5hm7pagDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task2:\n",
        "\n",
        "  col = task2[i]['col']\n",
        "  X = list(task2[i]['data'][col])\n",
        "  y = list(task2[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    RF_model = RandomForestClassifier()\n",
        "    RF_model.fit(X, y)\n",
        "    y_pred = RF_model.predict(X_test)\n",
        "    print(task2[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "kCUyFkNBaki9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest HPOs**"
      ],
      "metadata": {
        "id": "6QvmIg_Tal1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - RF"
      ],
      "metadata": {
        "id": "vGh3kJUAarQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF_parameters ={\n",
        "    'max_depth': [15,25],\n",
        "    'min_samples_split': [5,10],\n",
        "    'n_estimators': [200]}"
      ],
      "metadata": {
        "id": "_sTrIlokapp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task2:\n",
        "\n",
        "  col = task2[i]['col']\n",
        "  X = list(task2[i]['data'][col])\n",
        "  y = list(task2[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = RandomForestClassifier()\n",
        "    model = GridSearchCV(clf, RF_parameters, cv=4, scoring='accuracy',n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))\n"
      ],
      "metadata": {
        "id": "L4idUr2zawmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "Irab1_ZFayTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - RF"
      ],
      "metadata": {
        "id": "SraxX5XuazMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF_random ={\n",
        "    'max_depth': [35,45],\n",
        "    'min_samples_split': [5],\n",
        "    'n_estimators': [200]}"
      ],
      "metadata": {
        "id": "X-KVRg2dbGgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task2:\n",
        "\n",
        "  col = task2[i]['col']\n",
        "  X = list(task2[i]['data'][col])\n",
        "  y = list(task2[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = RandomForestClassifier()\n",
        "    model = RandomizedSearchCV(estimator = clf, param_distributions = RF_random, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))\n",
        "    "
      ],
      "metadata": {
        "id": "pK9VFmVMbIdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "a1yw-JosbLE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machine**"
      ],
      "metadata": {
        "id": "TGJYQYfQbMVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2dHe81xPbQwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks_selected_df = ros_data\n",
        "\n",
        "# train test spilt 80/20 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['bool_HIGH']), test_size=0.2, random_state=0)\n",
        "\n",
        "task3 = {\n",
        "    \"ks_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (SVM)'\n",
        "    },\n",
        "    \"kh_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_albert_vector',\n",
        "        'language_model' : 'ALBERT (SVM)'\n",
        "    },\n",
        "   \"ka_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_roberta_vector',\n",
        "        'language_model' : 'RoBERTa (SVM)'\n",
        "         },\n",
        "}"
      ],
      "metadata": {
        "id": "Iy58EKO0bT3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task3:\n",
        "\n",
        "  col = task3[i]['col']\n",
        "  X = list(task3[i]['data'][col])\n",
        "  y = list(task3[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    svm_model = svm.LinearSVC()\n",
        "    svm_model.fit(X, y)\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    print(task3[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "pHq_vzTAbVvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Support Vector Machine HPOs**"
      ],
      "metadata": {
        "id": "iZUKdP2Cbsvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - SVM"
      ],
      "metadata": {
        "id": "gHAZ1My0bWtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_grid ={\n",
        "    'C' : [0.01, 0.1, 1]}"
      ],
      "metadata": {
        "id": "gzdT8FoUbY_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task3:\n",
        "\n",
        "  col = task3[i]['col']\n",
        "  X = list(task3[i]['data'][col])\n",
        "  y = list(task3[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf =svm.LinearSVC()\n",
        "    model = GridSearchCV(clf, SVM_grid, cv=4, scoring='accuracy',n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "NC00bouDbaPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "tBnaAqXnbckS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - SVM"
      ],
      "metadata": {
        "id": "F31sCzrdbgSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_random ={\n",
        "    'C' : [0.01, 0.1, 1]}"
      ],
      "metadata": {
        "id": "lYlfd0mlbfIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task3:\n",
        "\n",
        "  col = task3[i]['col']\n",
        "  X = list(task3[i]['data'][col])\n",
        "  y = list(task3[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf =svm.LinearSVC()\n",
        "    model = model = RandomizedSearchCV(estimator = clf, param_distributions = SVM_random, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "NmAarj9xbkeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "P2-QX__TbnVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "D1JpYrqQbn7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JjV6-25tbrQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks_selected_df = ros_data\n",
        "\n",
        "# train test spilt 80/20 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['bool_HIGH']), test_size=0.2, random_state=0)\n",
        "\n",
        "task4 = {\n",
        "    \"ks_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (NB)'\n",
        "    },\n",
        "    \"kh_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_albert_vector',\n",
        "        'language_model' : 'ALBERT (NB)'\n",
        "    },\n",
        "   \"ka_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_roberta_vector',\n",
        "        'language_model' : 'RoBERTa (NB)'\n",
        "         },\n",
        "}"
      ],
      "metadata": {
        "id": "mO0SxH4pb1Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task4:\n",
        "\n",
        "  col = task4[i]['col']\n",
        "  X = list(task4[i]['data'][col])\n",
        "  y = list(task4[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    NB_model = GaussianNB()\n",
        "    NB_model.fit(X, y)\n",
        "    y_pred = NB_model.predict(X_test)\n",
        "    print(task4[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "CQMYjCKbb5LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naive Bayes HPOs**"
      ],
      "metadata": {
        "id": "EAKkdn7Rb6M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - NB"
      ],
      "metadata": {
        "id": "lJkj3k2scIOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_grid ={\n",
        "    'var_smoothing': (1e-01,1e-02,1e-03,1e-04,1e-05,1e-06,1e-07,1e-08,1e-09,1e-10,1e-11)}"
      ],
      "metadata": {
        "id": "2f0xKid2b9pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task4:\n",
        "\n",
        "  col = task4[i]['col']\n",
        "  X = list(task4[i]['data'][col])\n",
        "  y = list(task4[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = GaussianNB()\n",
        "    model = GridSearchCV(clf, NB_grid, cv=4, scoring='accuracy',n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "6eXsXqhucKpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "_qk_Z2-RcL9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - NB"
      ],
      "metadata": {
        "id": "xOQOnPOacNB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_random ={\n",
        "    'var_smoothing': (1e-01,1e-02,1e-03,1e-04,1e-05,1e-06,1e-07,1e-08,1e-09,1e-10,1e-11)}"
      ],
      "metadata": {
        "id": "nCm8zjdacP7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task4:\n",
        "\n",
        "  col = task4[i]['col']\n",
        "  X = list(task4[i]['data'][col])\n",
        "  y = list(task4[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = GaussianNB()\n",
        "    model =  RandomizedSearchCV(estimator = clf, param_distributions = NB_random, n_iter = 10, cv = 4, verbose= 2, random_state= 101, n_jobs = -1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "B5KrFSQkcROK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "Oh9mm95ncUA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "xYfvLcyvcUlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "u1vza5z-cWzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks_selected_df = ros_data\n",
        "\n",
        "# train test spilt 80/20 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['bool_HIGH']), test_size=0.2, random_state=0)\n",
        "\n",
        "task5 = {\n",
        "    \"ks_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (DT)'\n",
        "    },\n",
        "    \"kh_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_albert_vector',\n",
        "        'language_model' : 'ALBERT (DT)'\n",
        "    },\n",
        "   \"ka_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_roberta_vector',\n",
        "        'language_model' : 'RoBERTa (DT)'\n",
        "         },\n",
        "}"
      ],
      "metadata": {
        "id": "DToIrFgpcY1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task5:\n",
        "\n",
        "  col = task5[i]['col']\n",
        "  X = list(task5[i]['data'][col])\n",
        "  y = list(task5[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    DT_model = DecisionTreeClassifier(random_state=0)\n",
        "    DT_model.fit(X, y)\n",
        "    y_pred = DT_model.predict(X_test)\n",
        "    print(task5[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "B6wDT2gKcalt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decision Tree HPOs**"
      ],
      "metadata": {
        "id": "b-cq6SsFcb_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - DT"
      ],
      "metadata": {
        "id": "rldCH8FycjpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DT_grid ={\n",
        "    'max_depth': [30,35,40],\n",
        "    'min_samples_split': [5,10]}"
      ],
      "metadata": {
        "id": "x1V12Velcg60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task5:\n",
        "\n",
        "  col = task5[i]['col']\n",
        "  X = list(task5[i]['data'][col])\n",
        "  y = list(task5[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = DecisionTreeClassifier()\n",
        "    model = GridSearchCV(clf, DT_grid, cv=4, scoring='accuracy',n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "BSntfcV8cgCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "2rt_fVikcn32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - DT"
      ],
      "metadata": {
        "id": "9ovdldLpcocP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DT_random ={\n",
        "    'max_depth': [20,25,30],\n",
        "    'min_samples_split': [5,10]}"
      ],
      "metadata": {
        "id": "bMOs3gOOcq6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task5:\n",
        "\n",
        "  col = task5[i]['col']\n",
        "  X = list(task5[i]['data'][col])\n",
        "  y = list(task5[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = DecisionTreeClassifier()\n",
        "    model =  RandomizedSearchCV(estimator = clf, param_distributions = DT_random, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "cWhDRmQ5csLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "E3MX7V1_cuYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Nearest Neighbors**"
      ],
      "metadata": {
        "id": "V9nFfNqGcxUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Y03GQUFwc1fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task6:\n",
        "\n",
        "  col = task6[i]['col']\n",
        "  X = list(task6[i]['data'][col])\n",
        "  y = list(task6[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    KNN_model = KNeighborsClassifier(n_neighbors=3)\n",
        "    KNN_model.fit(X, y)\n",
        "    y_pred = KNN_model.predict(X_test)\n",
        "    print(task6[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "Wn0V03nsc2pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **K-Nearest Neighbors HPOs**"
      ],
      "metadata": {
        "id": "xLZjKK38c8EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - KNN"
      ],
      "metadata": {
        "id": "BjKDj19YdBxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_grid ={\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'n_neighbors': [1,2,3]}"
      ],
      "metadata": {
        "id": "BptL4wTZc6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task6:\n",
        "\n",
        "  col = task6[i]['col']\n",
        "  X = list(task6[i]['data'][col])\n",
        "  y = list(task6[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = KNeighborsClassifier()\n",
        "    model = GridSearchCV(clf, KNN_grid, cv=4, scoring='accuracy',n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "dOZkBzQkdFh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "8WLULCiJdHGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - KNN"
      ],
      "metadata": {
        "id": "In6ZVI1odI7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_random ={\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'n_neighbors': [1,2,3]}"
      ],
      "metadata": {
        "id": "miH-4pkBdLL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task6:\n",
        "\n",
        "  col = task6[i]['col']\n",
        "  X = list(task6[i]['data'][col])\n",
        "  y = list(task6[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 520\n",
        "\n",
        "  if is_manual:\n",
        "    clf = KNeighborsClassifier()\n",
        "    model =  RandomizedSearchCV(estimator = clf, param_distributions = KNN_random, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "    model.fit(X, y)\n",
        "    predictionforest = model.best_estimator_.predict(X_test)\n",
        "    print(confusion_matrix(y_test,predictionforest))\n",
        "    print(classification_report(y_test,predictionforest,digits = 4))"
      ],
      "metadata": {
        "id": "9jK-XbfidNMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "9HlPRJsPdPW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Experiments**"
      ],
      "metadata": {
        "id": "gbWUEOiOdRna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Under Sampling"
      ],
      "metadata": {
        "id": "NWLWJgt7ddxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ks_selected_df = ks_da_df\n",
        "\n",
        "# train test spilt 70/30 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['bool_HIGH']), test_size=0.2, random_state=0)\n",
        "\n",
        "data_zip = list(zip(ks_df_train['content_bert_vector'], ks_df_train['coffee_shop_name']))\n",
        "rus = RandomUnderSampler(random_state=777)\n",
        "rus_ks_df_train, rus_ks_df_train_y = rus.fit_resample(data_zip, ks_df_y_train)\n",
        "\n",
        "rus_ks_df_train_0 = [row[0] for row in rus_ks_df_train]\n",
        "\n",
        "ruc_bert = pd.DataFrame(list(zip(rus_ks_df_train_0, rus_ks_df_train_y)),\n",
        "               columns =['content_bert_vector', 'bool_HIGH'])\n",
        "\n",
        "\n",
        "data_zip = list(zip(ks_df_train['content_wangchanberta_vector'], ks_df_train['coffee_shop_name']))\n",
        "rus = RandomUnderSampler(random_state=777)\n",
        "rus_ks_df_train, rus_ks_df_train_y = rus.fit_resample(data_zip, ks_df_y_train)\n",
        "\n",
        "rus_ks_df_train_0 = [row[0] for row in rus_ks_df_train]\n",
        "\n",
        "ruc_wangchanberta = pd.DataFrame(list(zip(rus_ks_df_train_0, rus_ks_df_train_y)),\n",
        "               columns =['content_wangchanberta_vector', 'bool_HIGH'])\n",
        "\n",
        "\n",
        "\n",
        "data_zip = list(zip(ks_df_train['content_roberta_vector'], ks_df_train['coffee_shop_name']))\n",
        "rus = RandomUnderSampler(random_state=777)\n",
        "rus_ks_df_train, rus_ks_df_train_y = rus.fit_resample(data_zip, ks_df_y_train)\n",
        "\n",
        "rus_ks_df_train_0 = [row[0] for row in rus_ks_df_train]\n",
        "\n",
        "ruc_roberta = pd.DataFrame(list(zip(rus_ks_df_train_0, rus_ks_df_train_y)),\n",
        "               columns =['content_roberta_vector', 'bool_HIGH'])\n",
        "\n",
        "\n",
        "task_ruc = {\n",
        "    \"ks_df\": {\n",
        "        'data': ruc_bert,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (RandomUnderSampling)'\n",
        "    },\n",
        "    \n",
        "    \"kh_df\": {\n",
        "        'data': ruc_wangchanberta,\n",
        "        'col': 'content_wangchanberta_vector',\n",
        "        'language_model' : 'WangchanBERTa (RandomUnderSampling)'\n",
        "    },\n",
        "    \n",
        "    \"ka_df\": {\n",
        "        'data': ruc_roberta,\n",
        "        'col': 'content_roberta_vector',\n",
        "        'language_model' : 'RoBERTa (RandomUnderSampling)'\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "RHfla9pqddA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "\n",
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task_ruc:\n",
        "\n",
        "  col = task_ruc[i]['col']\n",
        "  X = list(task_ruc[i]['data'][col])\n",
        "  y = list(task_ruc[i]['data']['bool_HIGH'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['bool_HIGH'])\n",
        "\n",
        "  max_num_iter = 500\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    logreg_model = LogisticRegression(max_iter=max_num_iter, random_state=0,multi_class='multinomial')\n",
        "    logreg_model.fit(X, y)\n",
        "    y_pred = logreg_model.predict(X_test)\n",
        "    print(task_ruc[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "PwBzeIbbdVu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF Experiments**"
      ],
      "metadata": {
        "id": "RUHgPwbxdm1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "JggfE_PjdvtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Training and Testing Data\n",
        "train = pd.read_csv(path + '/ratings_and_sentiments UTF-8.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
        "test = pd.read_csv(path + '/ratings_and_sentiments UTF-8.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))"
      ],
      "metadata": {
        "id": "KgMuutTGdrmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Percentage of High/Low Sentiment\n",
        "print(\"High: \", train.bool_HIGH.value_counts()[1]/len(train)*100,\"%\")\n",
        "print(\"Low: \", train.bool_HIGH.value_counts()[0]/len(train)*100,\"%\")"
      ],
      "metadata": {
        "id": "_N2HVI61dt_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "_zqD9PAHdzXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter=PorterStemmer()\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "def review_text_cleaner(review_text):\n",
        "    soup = BeautifulSoup(review_text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "    # I will tokenize and join together to remove unneccessary white spaces\n",
        "    words = tok.tokenize(lower_case)\n",
        "    #Stemming\n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    words=\"\".join(stem_sentence).strip()\n",
        "    return words\n",
        "nums = [0,len(train)]\n",
        "clean_review_text = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    clean_review_text.append(review_text_cleaner(train['review_text'][i]))\n",
        "nums = [0,len(test)]\n",
        "test_review_text = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    test_review_text.append(review_text_cleaner(test['review_text'][i])) \n",
        "train_clean = pd.DataFrame(clean_review_text,columns=['review_text'])\n",
        "train_clean['bool_HIGH'] = train.bool_HIGH\n",
        "train_clean['coffee_shop_name'] = train.coffee_shop_name\n",
        "test_clean = pd.DataFrame(test_review_text,columns=['review_text'])\n",
        "test_clean['coffee_shop_name'] = test.coffee_shop_name"
      ],
      "metadata": {
        "id": "ivbPXwDBd3Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "JZCF8htQd5nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['review_text'],train_clean['bool_HIGH'])\n",
        "#label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "metadata": {
        "id": "qrozhR_rd7xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
        "tfidf_vect.fit(train_clean['review_text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "metadata": {
        "id": "zdRLhczWd9X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "txuZuloKeGJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "2iOzAt85eKa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf,valid_y)\n",
        "print (\"LR_Org\", accuracyORIGINAL)\n",
        "\n",
        "accuracyORIGINAL = train_model(RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
        "print (\"RF_Org\", accuracyORIGINAL)\n",
        "\n",
        "accuracyORIGINAL = train_model(svm.LinearSVC(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
        "print (\"SVM_Org\", accuracyORIGINAL)\n",
        "\n",
        "accuracyORIGINAL = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
        "print (\"NB_Org\", accuracyORIGINAL)\n",
        "\n",
        "accuracyORIGINAL = train_model(DecisionTreeClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
        "print (\"DT_Org\", accuracyORIGINAL)\n",
        "\n",
        "accuracyORIGINAL = train_model(KNeighborsClassifier(n_neighbors=3), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
        "print (\"KNN_Org\", accuracyORIGINAL)"
      ],
      "metadata": {
        "id": "0s0VhIRKeL8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Over Sampling"
      ],
      "metadata": {
        "id": "GNJisLYXePQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Over Sampling\n",
        "ros = RandomOverSampler(random_state=777)\n",
        "ros_xtrain_tfidf, ros_train_y = ros.fit_resample(xtrain_tfidf, train_y)\n",
        "\n",
        "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"LR_ROS\", accuracyROS)\n",
        "\n",
        "accuracyROS = train_model(RandomForestClassifier(),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"RF_ROS\", accuracyROS)\n",
        "\n",
        "accuracyROS = train_model(svm.LinearSVC(),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"SVM_ROS\", accuracyROS)\n",
        "\n",
        "accuracyROS = train_model(MultinomialNB(),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"NB_ROS\", accuracyROS)\n",
        "\n",
        "accuracyROS = train_model(DecisionTreeClassifier(random_state=0),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"DT_ROS\", accuracyROS)\n",
        "\n",
        "accuracyROS = train_model(KNeighborsClassifier(n_neighbors=3),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"KNN_ROS\", accuracyROS)"
      ],
      "metadata": {
        "id": "NF7vLd83eS39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF HPOs"
      ],
      "metadata": {
        "id": "hJgPpul9ebpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression"
      ],
      "metadata": {
        "id": "q45n5XAoe1iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - LR"
      ],
      "metadata": {
        "id": "fBNkrd4Xe_xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "cu6AHaJHfD1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_parameters ={\n",
        "    'C': [20,30,40]}  "
      ],
      "metadata": {
        "id": "ehsc8IeDe_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = LogisticRegression()\n",
        "model = GridSearchCV(clf, param_grid=LR_parameters, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "UpPiLoqee9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "6BYG3Xk9erxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - LR"
      ],
      "metadata": {
        "id": "A9BW4klVfGw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = LogisticRegression()\n",
        "model = RandomizedSearchCV(estimator = clf, param_distributions = LR_parameters, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "4JQo9klFelk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "NRBS_IGkfKwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "2p4iyhy8fMed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - RF"
      ],
      "metadata": {
        "id": "eO6bCA4AfPfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF_parameters ={\n",
        "    'max_depth': [50,75,100,125],\n",
        "    'min_samples_split': [5,10,15],\n",
        "    'n_estimators': [5,10,15,25]}"
      ],
      "metadata": {
        "id": "v47uCB6CfSJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = RandomForestClassifier()\n",
        "model = GridSearchCV(clf, param_grid=RF_parameters, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "gbcqgvEefTYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "Fn4T1a32fVG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - RF"
      ],
      "metadata": {
        "id": "mcbzuF7YfY-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF_random ={\n",
        "    'max_depth': [25,50,75,100,125,150],\n",
        "    'min_samples_split': [5,10,15],\n",
        "    'n_estimators': [5,10,15,25]}"
      ],
      "metadata": {
        "id": "CUxlshgffcIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = RandomForestClassifier()\n",
        "model = RandomizedSearchCV(estimator = clf, param_distributions = RF_random, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "0533VsUffeXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "38evc4NxfgEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "boeXzbUDfhdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - NB"
      ],
      "metadata": {
        "id": "zSQuOWBqflaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_TF ={\n",
        "    'var_smoothing': (0.001,0.01,0.1,1,10,100)}"
      ],
      "metadata": {
        "id": "bPj8vO1cfk9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = GaussianNB()\n",
        "model = GridSearchCV(clf, param_grid= NB_TF, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "model.fit(xtrain_tfidf.toarray(), train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf.toarray())\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "hvosJoh8foy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "PA3i7ke6fqUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - NB"
      ],
      "metadata": {
        "id": "pFQmRZZmfrRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = GaussianNB()\n",
        "model = RandomizedSearchCV(estimator = clf, param_distributions = NB_TF, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "model.fit(xtrain_tfidf.toarray(), train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf.toarray())\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "owTcu5e3fugf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "-y5tJXx7fvfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine"
      ],
      "metadata": {
        "id": "vCKv3jDNfw_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - NB"
      ],
      "metadata": {
        "id": "jJ-vaX_Af4iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_parameters ={\n",
        "    'C' : [0.1,1,10]}"
      ],
      "metadata": {
        "id": "AnOxo0hGf36g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = svm.LinearSVC()\n",
        "model = GridSearchCV(clf, param_grid= SVM_parameters, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "vAWnmFStf3MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "YY8f8bJ-f9DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - SVM"
      ],
      "metadata": {
        "id": "kHOeu5oNgB1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = svm.LinearSVC()\n",
        "model = RandomizedSearchCV(estimator = clf, param_distributions = SVM_parameters, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "wzLoHZSigChG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "sixSQVIBgEgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decision Tree"
      ],
      "metadata": {
        "id": "7daVk-aMgFn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DT_grid ={\n",
        "    'max_depth': [5,10,15,20],\n",
        "    'min_samples_split': [10,15,25]}"
      ],
      "metadata": {
        "id": "1XQJy07AgIka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = DecisionTreeClassifier()\n",
        "model = GridSearchCV(clf, param_grid= DT_grid, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "X9HCFE9CgKsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "r2Ku9KxygMSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Search - DT"
      ],
      "metadata": {
        "id": "S2rDi4zfgNez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DT_random ={\n",
        "    'max_depth': [5,10,15,20],\n",
        "    'min_samples_split': [10,15,25]}"
      ],
      "metadata": {
        "id": "GYVZDJ3ngOy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = DecisionTreeClassifier()\n",
        "model = RandomizedSearchCV(estimator = clf, param_distributions = DT_random, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "WZuMNZ6BgQUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "HmeGz2uugRLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "GaCXN10UgSUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search - KNN"
      ],
      "metadata": {
        "id": "onMcATxXgdiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_parameters ={\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'n_neighbors': [2,4,6,8,10,12,14,16,18,20]}"
      ],
      "metadata": {
        "id": "L22vb9f0gWdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = KNeighborsClassifier()\n",
        "model = GridSearchCV(clf, param_grid= KNN_parameters, cv=4, scoring='accuracy',error_score=0, n_jobs=-1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "sNhiiuZXgae6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "P--jGY9Egbro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random search - KNN"
      ],
      "metadata": {
        "id": "ho3DR_T7ggOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%time\n",
        "clf = KNeighborsClassifier()\n",
        "model = RandomizedSearchCV(estimator = clf, param_distributions = KNN_parameters, n_iter = 10, cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
        "model.fit(xtrain_tfidf, train_y)\n",
        "prediction = model.best_estimator_.predict(xvalid_tfidf)\n",
        "print(confusion_matrix(valid_y,prediction))\n",
        "print(classification_report(valid_y,prediction,digits = 4))"
      ],
      "metadata": {
        "id": "w6zxRKjIgjK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "IoTXZ_J3gkmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Experiments for TF-IDF"
      ],
      "metadata": {
        "id": "E1Nwsga-glZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE one of imbalance data handling technique\n",
        "sm = SMOTE(random_state=777) #, ratio = 1.0)\n",
        "sm_xtrain_tfidf, sm_train_y = sm.fit_resample(xtrain_tfidf, train_y)\n",
        "\n",
        "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"LR_SM\", accuracySMOTE)\n",
        "accuracySMOTE = train_model(svm.LinearSVC(),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"SVM_SM\", accuracySMOTE)\n",
        "\n",
        "accuracySMOTE = train_model(RandomForestClassifier(),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf,valid_y)\n",
        "print (\"RF_SM\", accuracySMOTE)"
      ],
      "metadata": {
        "id": "DyGThGDYgpll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}